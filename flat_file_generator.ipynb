{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0939cf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import re\n",
    "\n",
    "\n",
    "def extract_float(s):\n",
    "    # This regex pattern looks for any sequence of digits (\\d+), optionally followed by\n",
    "    # a decimal point and more digits (\\.\\d+)? The entire pattern is wrapped in parentheses\n",
    "    # to capture the match as a group.\n",
    "    match = re.search(r'(\\d+(\\.\\d+)?)', s)\n",
    "    if match:\n",
    "        return float(match.group(0))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def flatten_dict(d, parent_key='', sep='.'):\n",
    "    \"\"\"\n",
    "    Recursively flattens a nested dictionary and concatenates keys.\n",
    "    \"\"\"\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "\n",
    "\n",
    "def read_yaml_files(directory):\n",
    "    data = []\n",
    "    unique_keys = set()\n",
    "    # Iterate through each file in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.yaml') or filename.endswith('.yml'):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            with open(filepath, 'r') as file:\n",
    "                # Load and flatten the YAML file\n",
    "                yaml_data = yaml.safe_load(file)\n",
    "                flattened_data = flatten_dict(yaml_data)\n",
    "                data.append(flattened_data)\n",
    "                # Update unique keys set\n",
    "                unique_keys.update(flattened_data.keys())\n",
    "    return data, unique_keys\n",
    "\n",
    "def write_to_flat_file(data, unique_keys, output_file):\n",
    "    with open(output_file, 'w') as file:\n",
    "        # Write the header\n",
    "        headers = list(unique_keys)\n",
    "        headers.sort()  # Optionally sort the headers for consistent ordering\n",
    "        file.write('\\t'.join(headers) + '\\n')\n",
    "        # Write the data\n",
    "        for item in data:\n",
    "            row = []\n",
    "            for key in headers:\n",
    "                row.append(str(item.get(key, \"\")))\n",
    "            file.write('\\t'.join(row) + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "443fabbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '/projectnb/textconv/distill/mdistiller/configs/cifar100/_sldmse/r56r20/'\n",
    "output_file = 'sample_out.tsv'\n",
    "\n",
    "# Process the YAML files\n",
    "data, unique_keys = read_yaml_files(directory)\n",
    "\n",
    "# Write the collected data to a flat file\n",
    "write_to_flat_file(data, unique_keys, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "297f5e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def add_best_acc_to_file(input_file, output_file, directory_prefix):\n",
    "    # Load the tab-delimited file into a DataFrame\n",
    "    df = pd.read_csv(input_file, delimiter='\\t')\n",
    "    \n",
    "    # Ensure the EXPERIMENT.NAME column exists\n",
    "    if 'EXPERIMENT.NAME' not in df.columns:\n",
    "        raise ValueError(\"EXPERIMENT.NAME column not found in the input file.\")\n",
    "    \n",
    "    # Initialize the BEST_ACC column with NaNs (or a default value of your choice)\n",
    "    df['BEST_ACC'] = float('nan')\n",
    "    \n",
    "    # Iterate over the rows in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        experiment_name = str(row['EXPERIMENT.NAME'])\n",
    "        folder_path = os.path.join(directory_prefix, experiment_name)\n",
    "        worklog_path = os.path.join(folder_path, 'worklog.txt')\n",
    "        \n",
    "        try:\n",
    "            # Attempt to open and read the worklog.txt file\n",
    "            with open(worklog_path, 'r') as worklog_file:\n",
    "                for line in worklog_file:\n",
    "                    # Look for the line starting with 'best_acc'\n",
    "                    if line.startswith('best_acc'):\n",
    "                        # Extract the value after 'best_acc' and update the DataFrame\n",
    "                        best_acc_value = extract_float(line.split()[-1])  # Assuming the value is the last item on the line\n",
    "                        \n",
    "                        df.at[index, 'BEST_ACC'] = best_acc_value\n",
    "                        break\n",
    "        except FileNotFoundError:\n",
    "            # Handle the case where the worklog.txt file does not exist\n",
    "            print(f\"worklog.txt not found for {experiment_name} in {folder_path}\")\n",
    "    \n",
    "    # Save the updated DataFrame to a new tab-delimited file\n",
    "    df.to_csv(output_file, sep='\\t', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "340643e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worklog.txt not found for SLDMSE_R56R20XDS2 in /projectnb/textconv/distill/mdistiller/output/cifar100_baselines/SLDMSE_R56R20XDS2\n",
      "worklog.txt not found for SLDMSE_R56R20XDS1 in /projectnb/textconv/distill/mdistiller/output/cifar100_baselines/SLDMSE_R56R20XDS1\n"
     ]
    }
   ],
   "source": [
    "input_file = 'sample_out.tsv'  # The path to your tab-delimited flat file\n",
    "from datetime import datetime\n",
    "\n",
    "# Get the current date and time\n",
    "current_datetime = datetime.now()\n",
    "\n",
    "# Format the date and time in a way that is safe for filenames\n",
    "filename_format = current_datetime.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "dir_prefix = \"_\".join(directory.split('/')[6:])\n",
    "filename_format = dir_prefix + filename_format\n",
    "\n",
    "output_file = f'sample_out_updated_{filename_format}.tsv'  # The path for the output file with the BEST_ACC column\n",
    "directory_prefix = '/projectnb/textconv/distill/mdistiller/output/cifar100_baselines/'  # The prefix to the directory containing experiment folders\n",
    "\n",
    "# Call the function with the specified paths\n",
    "add_best_acc_to_file(input_file, output_file, directory_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41494fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
